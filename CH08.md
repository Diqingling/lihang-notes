# CH08 提升方法

## 8.2 AdaBoost算法的训练误差分析

定理8.2中：

$\gamma_m=\frac{1}{2}-e_m$可以理解为一个分类器对于随机分类结果的提升程度。$\gamma_m$越大，AdaBoost的训练误差上界越小。

## 8.3.2 前向分布算法与AdaBoost

* 为什么$ G^*_m(x)=\arg\min \limits_ {G}\sum_{i=1}^{N} \overline w_{mi}I(G_m(x_i)\ne y_i)$

  实际上也是由式（8.22）得到的，在式（8.22）中去掉与$\alpha$有关的项，可以得到
  $$
  \begin{aligned}
  G^*_m(x)= &\arg\min \limits_ {G}\sum_{i=1}^N \overline w_{mi}exp(-\alpha y_iG(x_i))\\
  =& \arg\min \limits_ {G}\sum_{i=1}^N \overline w_{mi} \\
  =& \arg\min \limits_ {G}\sum_{i=1}^{N} \overline w_{mi}I(G_m(x_i)\ne y_i)
  \end{aligned}
  $$

* $\alpha_m=\frac{1}{2}log\frac{1-e_m}{e_m}$的另一种理解：

  由定理8.1，AdaBoost的训练误差界可以被$\prod\limits_m Z_m$控制住，因此最小化训练误差可以通过最小化$\prod\limits_m Z_m$实现

  式（8.5）$Z_m=\sum_{i=1}^Nw_{mi}exp(-\alpha_my_iG_m(x_i))$中，已知$w_{mi}$和$G_m(x_i)$,则$\alpha_m=\arg\min Z_m$
  $$
  \frac{\partial Z_m}{\partial \alpha_m}=\sum_{i=1}^N-w_{mi}y_iG_m(x_i)exp(-\alpha_my_iG_m(x_i))
  $$
  将$\alpha_m=\frac{1}{2}log\frac{1-e_m}{e_m}$代入上式可使$\frac{\partial Z_m}{\partial \alpha_m}=0$		

* 为什么$w_{m+1,i}=\overline w_{m,i}exp(-\alpha_my_iG_m(x_i))$：
  $$
  \overline w_{m,i}=exp(-y_if_{m-1}(x))=exp(-y_i\sum_{j=1}^{m-1}\alpha_j G_j(x_i))=\prod\limits_j exp(y_i\alpha_j G_j(x_i))
  $$
  所以$w_{m+1,i}=\overline w_{m,i}exp(-\alpha_my_iG_m(x_i))$

## 8.4.3 梯度提升

算法8.4(2)(b)步求解了每个分类器每个叶结点的区域$R_{mj}$

8.4(2)(c)步求解了每个分类器每个叶结点的输出
$$
c_{mj}=\arg\min_c\sum_{xi\in R_{mj}}L(y_i,f_{m-1}(x_i)+c)
$$
$xi\in R_{mj}$表示对于每个叶结点，只考虑被分到该结点的样本点$x_i$。使用这些样本点求解使该叶结点的损失函数$L(y_i,f_{m-1}(x_i)+c)$最小的$c$

8.4(2)(d))步更新回归树
$$
f_m(x)=f_{m-1}(x)+\sum_{j=1}^Jc_{mj}I(x\in R_{mj})
$$
该式虽有$j$个求和项，但对于每个样本点$x_i$，只有一项为1，其余都为0





