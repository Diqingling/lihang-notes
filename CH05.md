# CH05 决策树

### 5.2.2 信息增益

机器学习5-决策树中一些概念的补充

1. **信息熵**

1. **条件熵**

1. **联合熵(相当于并集)**

   $H(X, Y) = H(X) + H(Y|X) = H(Y)+H(X|Y) = H(X|Y)+H(Y|X)+I(X;Y)$

1. **互信息 (信息增益，相当于交集)**

   信息增益等价于互信息。可以看成由于知道$y$值而造成的$x$的不确定性的减小(反之亦然)。
   $$
   I(x,y)=H(X)-H(X|Y)=H(Y)-H(Y|X)
   $$
   ![](pics/5.1.png)
   
1. **相对熵 (KL 散度)** 

   相对熵(Relative Entropy)描述差异性，从分布的角度描述差异性，可用于度量两个概率分布之间的差异。

   > 考虑由$p(x,y)$给出的两个变量$x$和$y$组成的数据集。如果变量的集合是独立的，那么他们的联合分布可以分解为边缘分布的乘积$p(x,y)=p(x)p(y)$
   >
   > 如果变量不是独立的，那么我们可以通过考察**联合分布**与**边缘分布乘积**之间的KL散度来判断他们是否"接近"于相互独立。
   >
   > $$I(x,y)=KL(p(x,y)|p(x)p(y))=-\iint p(x,y) \ln {\left( \frac{p(x)p(y)}{p(x,y)}\right)}$$
   >
   
   

1. **交叉熵**

   刻画两个分布之间的差异
   $$
   \begin{aligned}
   CH(p,q)&=-\sum\limits_{i=1}^{n}p(x_i)\log{q(x_i)}\\
   &=-\sum\limits_{i=1}^{n}p(x_i)\log{p(x_i)}+\sum\limits_{i=1}^{n}p(x_i)\log{p(x_i)}-\sum\limits_{i=1}^{n}p(x_i)\log{q(x_i)}\\
   &=H(p)+\sum\limits_{i=1}^{n}p(x_i)\log{\frac{p(x_i)}{q(x_i)}}\\
   &=H(p)+KL(p||q)
   \end{aligned}
   $$

   





