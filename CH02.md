# CH02 感知机
### 2.2.2 感知机学习策略 

关于输入空间$R^n$中任一点$x_0$到超平面的距离$S$的推导，可参考模式识别 4.2

### 2.3.3 感知机学习算法的对偶形式

假设样本点$(x_i,y_i)$在更新过程中被使用了$n_i$次。因此，从原始形式的学习过程可以得到，最后学习到的$w$和$b$可以分别表示为：
$$
w=\sum_{i=1}^Nn_i\eta y_ix_i\\b=\sum_{i=1}^Nn_i\eta y_i
$$


考虑$n_i$的含义：如果$n_i$的值越大，那么意味着这个样本点经常被误分。什么样的样本点容易被误分？很明显就是离超平面很近的点。超平面稍微一点点移动，这个点就从正变为负，或者从负变为正。如果学过SVM就金发现，这种点很可能就是支持向量。
代入式（1）和式（2）到原始形式的感知机模型中，可得：
$$
f(x)=sign\left(w\cdot x+b\right)=sign\left(\sum_{j=1}^Nn_j\eta y_jx_j\cdot x+\sum_{j=1}^Nn_j\eta y_j\right)\nonumber
$$

此时，学习的目标就不再是$w$和$b$，而是$n_i$，$i=1，2.…，N$。
相应地，训练过程变为：
1.初始时${\forall}n_i=0$。
2.在训练集中选取数据$(x_i,y_i)$
3.如果$y_i\left(\sum_{j=1}^Nn_j\eta y_jx_j\cdot x_i+\sum_{j=1}^Nn_j\eta y_j\right)≤0$，更新：$n_i\leftarrow n_i+1$。转至2直至没有误分类数据。
可以看出，其实对偶形式和原始形式并没有本质区别，那么对偶形式的意义在哪里呢？从式（3）中可以看出，样本点的特征向星以内积的形式存在于感知机对偶形式的训练算法中，因此，如果事先计算好所有的内积，也就是Gram矩阵，就可以大大地加快计算速度。